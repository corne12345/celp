{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_city(city_name):\n",
    "    \"\"\"\n",
    "    Load all the information of a city into 5 dataframes\n",
    "    \"\"\"\n",
    "    lst=[]\n",
    "    for file in ['business', 'checkin', 'review', 'tip', 'user']:\n",
    "        f = open(f'yelp-all/{city_name}/{file}.json')\n",
    "        lst.append(pd.read_json(f, lines = True))\n",
    "\n",
    "    business = lst[0]\n",
    "    checkin = lst[1]\n",
    "    review = lst[2]\n",
    "    tip = lst[3]\n",
    "    user = lst[4]\n",
    "    return business, checkin, review, tip, user\n",
    "\n",
    "def just_categories(business):\n",
    "    \"\"\"\n",
    "    Split the  categories column of a dataframe into seperate categories with a seperate entry with combination category\n",
    "    and business_id as key.\n",
    "    \"\"\"\n",
    "    genres_m = business.apply(lambda row: pd.Series([row['business_id']] + row['categories'].lower().split(\",\")), axis=1)\n",
    "    stack_genres = genres_m.set_index(0).stack()\n",
    "    df_stack_genres = stack_genres.to_frame()\n",
    "    df_stack_genres['business_id'] = stack_genres.index.droplevel(1)\n",
    "    df_stack_genres.columns = ['category', 'business_id']\n",
    "    return df_stack_genres.reset_index()[['business_id', 'category']]\n",
    "\n",
    "def split_data(data, d=0.75):\n",
    "    \"\"\"\n",
    "    Split data in a training and test set with a standard distrubution of 0.75\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=5)\n",
    "    mask_test = np.random.rand(data.shape[0]) < d\n",
    "    mask_test = mask_test\n",
    "    return data[mask_test], data[~mask_test]\n",
    "\n",
    "def handle_duplicates (reviews):\n",
    "    \"\"\"\n",
    "    returns the mean of businesses having multiple reviews by the same user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return reviews.groupby(['business_id', 'categories', 'user_id'])['stars'].mean().reset_index()\n",
    "    except:\n",
    "        return reviews.groupby(['business_id', 'user_id'])['stars'].mean().reset_index()\n",
    "\n",
    "def join_business_reviews(business, reviews):\n",
    "    business = business.set_index('business_id')\n",
    "    reviews = reviews.set_index('business_id')\n",
    "    temp = reviews.join(business, rsuffix='business').reset_index()\n",
    "    return temp[['user_id', 'business_id', 'review_id', 'categories', 'stars']]\n",
    "    \n",
    "def rating_density(reviews):\n",
    "    \"\"\"\n",
    "    Compute the density of a dataset\n",
    "    \"\"\"\n",
    "    return number_of_ratings(reviews)/(number_of_businesses(reviews) * number_of_users(reviews))\n",
    "\n",
    "def number_of_businesses(reviews):\n",
    "    \"\"\"\n",
    "    returns the number of unique businesses in a set of reviews\n",
    "    \"\"\"\n",
    "    return len(reviews['business_id'].unique())\n",
    "\n",
    "def number_of_users(reviews):\n",
    "    \"\"\"\n",
    "    returns the number of unique users in a set of reviews\n",
    "    \"\"\"\n",
    "    return len(reviews['user_id'].unique())\n",
    "\n",
    "def number_of_ratings(reviews):\n",
    "    \"\"\"\n",
    "    returns the number of ratings of a set of reviews\n",
    "    \"\"\"\n",
    "    return reviews.shape[0]\n",
    "\n",
    "def print_properties(reviews):\n",
    "    print(\"Number of Businesses\", number_of_businesses(reviews))\n",
    "    print(\"Number of Users\", number_of_users(reviews))\n",
    "    print(\"Number of ratings\", number_of_ratings(reviews))\n",
    "    print(\"Rating Density\", rating_density(reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_categories(df):\n",
    "    \"\"\"\n",
    "    Create a one-hot encoded matrix for genres\n",
    "    \"\"\"\n",
    "    return df.pivot_table(index='business_id', columns='category', aggfunc='size', fill_value=0)\n",
    "\n",
    "def pivot_ratings(df):\n",
    "    \"\"\"\n",
    "    Creates a utility matrix for user ratings for businesses\n",
    "    \"\"\"\n",
    "    return df.pivot(values='stars', columns='user_id', index='business_id')\n",
    "\n",
    "def create_similarity_matrix_categories(matrix):\n",
    "    \"\"\"\n",
    "    Create a similarity matrix based on categories\n",
    "    \"\"\"\n",
    "    npu = matrix.values\n",
    "    m1 = npu @ npu.T\n",
    "    diag = np.diag(m1)\n",
    "    m2 = m1 / diag\n",
    "    m3 = np.minimum(m2, m2.T)\n",
    "    return pd.DataFrame(m3, index = matrix.index, columns = matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(similarity, utility, to_predict):\n",
    "    \"\"\"Predicts the predicted rating for the input test data.\n",
    "    \n",
    "    Arguments:\n",
    "    similarity -- a dataFrame that describes the similarity between items\n",
    "    utility    -- a dataFrame that contains a rating for each user (columns) and each movie (rows). \n",
    "                  If a user did not rate an item the value np.nan is assumed. \n",
    "    to_predict -- A dataFrame containing at least the columns movieId and userId for which to do the predictions\n",
    "    \"\"\"\n",
    "    # copy input (don't overwrite)\n",
    "    ratings_test_c = to_predict.copy()\n",
    "    # apply prediction to each row\n",
    "    ratings_test_c['predicted rating'] = to_predict.apply(lambda row: predict_ids(similarity, utility, row['user_id'], row['business_id']), axis=1)\n",
    "    return ratings_test_c[['business_id', 'user_id', 'stars', 'predicted rating']]\n",
    "\n",
    "### Helper functions for predict_ratings_item_based ###\n",
    "\n",
    "def predict_ids(similarity, utility, user_id, business_id):\n",
    "    # select right series from matrices and compute\n",
    "    if user_id in utility.columns and business_id in similarity.index:\n",
    "        return predict_vectors(utility.loc[:,user_id], similarity[business_id])\n",
    "    return 0\n",
    "\n",
    "def predict_vectors(user_ratings, similarities):\n",
    "    # select only movies actually rated by user\n",
    "    relevant_ratings = user_ratings.dropna()\n",
    "    \n",
    "    # select corresponding similairties\n",
    "    similarities_s = similarities[relevant_ratings.index]\n",
    "    \n",
    "    # select neighborhood\n",
    "    similarities_s = similarities_s[similarities_s > 0.0]\n",
    "    relevant_ratings = relevant_ratings[similarities_s.index]\n",
    "    \n",
    "    # if there's nothing left return a prediction of 0\n",
    "    norm = similarities_s.sum()\n",
    "    if(norm == 0):\n",
    "        return 0\n",
    "    \n",
    "    # compute a weighted average (i.e. neighborhood is all) \n",
    "    return np.dot(relevant_ratings, similarities_s)/norm\n",
    "\n",
    "def mse(predicted_ratings):\n",
    "    \"\"\"\n",
    "    Computes the means square error betweeen actual ratings \n",
    "    \"\"\"\n",
    "    diff = predicted_ratings['stars'] - predicted_ratings['predicted rating']\n",
    "    return (diff**2).mean()\n",
    "\n",
    "def mean_center_columns(matrix):\n",
    "    for column in matrix.columns:\n",
    "        matrix[column] -= matrix[column].mean()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(matrix, id1, id2):\n",
    "    \"\"\"Compute cosine similarity\"\"\"\n",
    "    selected_features = matrix.loc[id1].notna() & matrix.loc[id2].notna()\n",
    "    \n",
    "    # if no matching features, return 0\n",
    "    if not selected_features.any():\n",
    "        return 0\n",
    "    \n",
    "    # get the features from the matrix\n",
    "    features1 = matrix.loc[id1][selected_features]\n",
    "    features2 = matrix.loc[id2][selected_features]\n",
    "    \n",
    "    # return 1 for the diagonals and 0 if there are no matching features\n",
    "    if features1.equals(features2):\n",
    "        return 1\n",
    "    if features1.max() == 0 or features2.max() == 0:\n",
    "        return 0\n",
    "    \n",
    "    return sum(features1 * features2)/((sum(features1**2)**0.5) * sum((features2**2))**0.5)\n",
    "\n",
    "\n",
    "def create_similarity_matrix_cosine(matrix):\n",
    "    \"\"\" creates the similarity matrix based on cosine similarity \"\"\"\n",
    "    similarity_matrix = pd.DataFrame(0, index=matrix.index, columns=matrix.index, dtype=float)\n",
    "    id1 = similarity_matrix.columns.values\n",
    "    for i in id1:\n",
    "        for j in id1:\n",
    "            similarity_matrix[i][j] = cosine_similarity(matrix, i, j)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_benchmark(city):\n",
    "    \"\"\"\n",
    "    Test the performance of our content-based system\n",
    "    \"\"\"\n",
    "    a, b, c, d, e = load_city(city)\n",
    "    print(city, \"Content-based collaborative filtering\")\n",
    "    print_properties(c)\n",
    "    review_business = join_business_reviews(a,c)\n",
    "    review_business_clean = handle_duplicates(review_business)\n",
    "    training, test = split_data(review_business_clean, d=0.9)\n",
    "    training_split = just_categories(training).drop_duplicates()\n",
    "    utility_categories = pivot_categories(training_split)\n",
    "    utility_ratings = pivot_ratings(review_business_clean)\n",
    "    similarity_categories = create_similarity_matrix_categories(utility_categories)\n",
    "    predictions = predict_ratings(similarity_categories, utility_ratings, test[['user_id', 'business_id', 'stars']])\n",
    "    print(\"MSE\", mse(predictions), end='\\n\\n')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_benchmark(city):\n",
    "    \"\"\"\n",
    "    Test the performance of our user-based system\n",
    "    \"\"\"\n",
    "    a, b, c, d, e = load_city(city)\n",
    "    print(city, \"User-based collaborative filtering\")\n",
    "    print_properties(c)\n",
    "    reviews_clean = handle_duplicates(c)\n",
    "    training, test = split_data(reviews_clean, d=0.9)\n",
    "    utility = mean_center_columns(pivot_ratings(training))\n",
    "    similarity = create_similarity_matrix_cosine(utility)\n",
    "    predictions = predict_ratings(similarity, utility, test[['user_id', 'business_id', 'stars']])\n",
    "    print(\"MSE\", mse(predictions), end='\\n\\n')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_benchmark(city):\n",
    "    \"\"\"\n",
    "    Test the performance of our user-based system\n",
    "    \"\"\"\n",
    "    a, b, c, d, e = load_city(city)\n",
    "    print(city, \"Item-based collaborative filtering\")\n",
    "    print_properties(c)\n",
    "    reviews_clean = handle_duplicates(c)\n",
    "    training, test = split_data(reviews_clean, d=0.9)\n",
    "    utility = mean_center_columns(pivot_ratings(training).T).T\n",
    "    similarity = create_similarity_matrix_cosine(utility)\n",
    "    predictions = predict_ratings(similarity, utility, test[['user_id', 'business_id', 'stars']])\n",
    "    print(\"MSE\", mse(predictions), end='\\n\\n')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stouffville Content-based collaborative filtering\n",
      "Rating Density 0.02642570281124498\n",
      "MSE 0.12250452950721911\n",
      "\n",
      "stouffville User-based collaborative filtering\n",
      "Rating Density 0.02642570281124498\n",
      "MSE 12.367245558057054\n",
      "\n",
      "stouffville Item-based collaborative filtering\n",
      "Rating Density 0.02642570281124498\n",
      "MSE 11.695756678713476\n",
      "\n",
      "sun city Content-based collaborative filtering\n",
      "Rating Density 0.0048678109581154734\n",
      "MSE 0.040468525854130265\n",
      "\n",
      "sun city User-based collaborative filtering\n",
      "Rating Density 0.0048678109581154734\n",
      "MSE 14.585720771152895\n",
      "\n",
      "sun city Item-based collaborative filtering\n",
      "Rating Density 0.0048678109581154734\n",
      "MSE 14.580341553255334\n",
      "\n",
      "westlake Content-based collaborative filtering\n",
      "Rating Density 0.0043414322968397595\n",
      "MSE 0.12593121629337964\n",
      "\n",
      "westlake User-based collaborative filtering\n",
      "Rating Density 0.0043414322968397595\n"
     ]
    }
   ],
   "source": [
    "for city in ['stouffville', 'sun city', 'westlake']:\n",
    "    content_based_benchmark(city)\n",
    "    user_based_benchmark(city)\n",
    "    item_based_benchmark(city)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
